{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, MaxAbsScaler, PowerTransformer, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, make_scorer, average_precision_score, recall_score, precision_score, f1_score, ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, SelectFromModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_MB = \"Charangan/MedBERT\"\n",
    "tokenizer_MB = AutoTokenizer.from_pretrained(model_name_MB)\n",
    "model_MB = AutoModel.from_pretrained(model_name_MB)\n",
    "model_MB.eval()\n",
    "\n",
    "def get_embedding_MB(text, feature_value):\n",
    "    inputs = tokenizer_MB(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_MB(**inputs)\n",
    "    text_embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy().squeeze()\n",
    "    feature_value_array = np.array([feature_value], dtype=np.float32)\n",
    "    combined_embedding = np.concatenate([text_embeddings, feature_value_array])\n",
    "    return combined_embedding.astype(np.float32)\n",
    "\n",
    "\n",
    "model_name_CB = \"medicalai/ClinicalBERT\"\n",
    "tokenizer_CB = AutoTokenizer.from_pretrained(model_name_CB)\n",
    "model_CB = AutoModel.from_pretrained(model_name_CB)\n",
    "\n",
    "model_CB.eval()\n",
    "\n",
    "def get_embedding_CB(text, feature_value):\n",
    "    inputs = tokenizer_CB(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_CB(**inputs)\n",
    "    text_embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy().squeeze()\n",
    "    feature_value_array = np.array([feature_value], dtype=np.float32)\n",
    "    combined_embedding = np.concatenate([text_embeddings, feature_value_array])\n",
    "    return combined_embedding.astype(np.float32)\n",
    "\n",
    "Model_name_Bert = \"google-bert/bert-base-uncased\"\n",
    "tokenizer_Bert = AutoTokenizer.from_pretrained(Model_name_Bert)\n",
    "model_Bert = AutoModel.from_pretrained(Model_name_Bert)\n",
    "\n",
    "\n",
    "model_Bert.eval()\n",
    "\n",
    "def get_embedding_Bert(text, feature_value):\n",
    "    inputs = tokenizer_Bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_Bert(**inputs)\n",
    "    text_embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy().squeeze()\n",
    "    feature_value_array = np.array([feature_value], dtype=np.float32)\n",
    "    combined_embedding = np.concatenate([text_embeddings, feature_value_array])\n",
    "    return combined_embedding.astype(np.float32)\n",
    "\n",
    "Model_name_BB = \"dmis-lab/biobert-v1.1\"\n",
    "tokenizer_BB = AutoTokenizer.from_pretrained(Model_name_BB)\n",
    "model_BB = AutoModel.from_pretrained(Model_name_BB)\n",
    "\n",
    "model_BB.eval()\n",
    "\n",
    "def get_embedding(text, feature_value, method=\"cls\"):\n",
    "    inputs = tokenizer_BB(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_BB(**inputs)\n",
    "\n",
    "    hidden_states = outputs.last_hidden_state  # shape: (1, seq_len, hidden_dim)\n",
    "\n",
    "    if method == \"cls\":\n",
    "        # CLS token embedding\n",
    "        embedding = hidden_states[:, 0, :]  # shape: (1, hidden_dim)\n",
    "        text_embedding = embedding.squeeze(0).cpu().numpy()  # shape: (hidden_dim,)\n",
    "    elif method == \"mean\":\n",
    "        # Mean pooling over tokens, masking out padding\n",
    "        input_mask_expanded = inputs['attention_mask'].unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        embedding = sum_embeddings / sum_mask  # shape: (1, hidden_dim)\n",
    "        text_embedding = embedding.squeeze(0).cpu().numpy()  # shape: (hidden_dim,)\n",
    "    #elif method == \"token\":\n",
    "        #return hidden_states.squeeze(0).cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    # Ensure scalar feature_value is valid float\n",
    "    try:\n",
    "        feature_value_float = float(feature_value)\n",
    "    except (ValueError, TypeError):\n",
    "        feature_value_float = 0.0\n",
    "\n",
    "    feature_value_array = np.array([feature_value_float], dtype=np.float32)\n",
    "    combined_embedding = np.concatenate([text_embedding, feature_value_array])\n",
    "    return combined_embedding.astype(np.float32)\n",
    "\n",
    "def embedd_text(text):\n",
    "    inputs = tokenizer_BB(text, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_BB(**inputs)\n",
    "\n",
    "    # Get all token embeddings\n",
    "    token_embeddings = outputs.last_hidden_state[:, 0, :]  # shape: (seq_len, hidden_dim)\n",
    "    \n",
    "    return token_embeddings.numpy().squeeze()\n",
    "\n",
    "def get_word_level_embedding(text, feature_value):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer_BB(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_BB(**inputs)\n",
    "\n",
    "    # Get all token embeddings\n",
    "    token_embeddings = outputs.last_hidden_state.squeeze(0)  # shape: (seq_len, hidden_dim)\n",
    "    \n",
    "    # Get attention mask and remove [CLS] and [SEP] tokens (usually first and last)\n",
    "    attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "    valid_token_embeddings = token_embeddings[attention_mask == 1][1:-1]  # exclude CLS/SEP\n",
    "\n",
    "    # Average over valid tokens\n",
    "    mean_token_embedding = valid_token_embeddings.mean(dim=0).cpu().numpy()\n",
    "\n",
    "    # Append the scalar feature\n",
    "    feature_value_array = np.array([feature_value], dtype=np.float32)\n",
    "    combined_embedding = np.concatenate([mean_token_embedding, feature_value_array])\n",
    "    return combined_embedding.astype(np.float32)\n",
    "\n",
    "\n",
    "def stack_embeddings(group):\n",
    "    return np.mean(np.stack(group['embedding'].tolist()), axis=0)\n",
    "\n",
    "def save_embeddings(df, embeddings, output_path):\n",
    "    df['embedding'] = [e.astype(np.float32) for e in embeddings]\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created directory: {output_dir}\")\n",
    "    grouped_df = df.groupby(\"patient_id\", group_keys=False)[[\"embedding\"]].apply(stack_embeddings).reset_index()\n",
    "    grouped_df.columns = [\"patient_id\", \"combined_embedding\"]\n",
    "\n",
    "    # Sanity check before saving\n",
    "    grouped_df[\"combined_embedding\"] = grouped_df[\"combined_embedding\"].apply(lambda x: json.dumps(x.tolist()))\n",
    "    lengths = grouped_df[\"combined_embedding\"].apply(lambda x: len(json.loads(x)))\n",
    "    print(\"Embedding length distribution:\", lengths.value_counts())\n",
    "    grouped_df.to_csv(output_path, index=False)\n",
    "    print(f\"Embeddings saved to {output_path}\")\n",
    "\n",
    "\n",
    "def fit_logistic_regression(X, y,):\n",
    "    model = LogisticRegression(solver='liblinear', max_iter=5000, class_weight='balanced')\n",
    "    return model.fit(X, y)\n",
    "\n",
    "\n",
    "def find_best_threshold(y_true, y_pred_probs):\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    best_threshold = 0.5\n",
    "    best_balanced_acc = -1\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "        bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "        if bal_acc > best_balanced_acc:\n",
    "            best_balanced_acc = bal_acc\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold\n",
    "\n",
    "def metrics(y_test, y_pred_probs, threshold):\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_probs)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    sensitivity = recall_score(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    return bal_acc, auc, f1, sensitivity, specificity\n",
    "\n",
    "def bootstrap_metrics(X_train, X_test, y_train, y_test, threshold, n_bootstrap=1000):\n",
    "    \"\"\"Performs bootstrap resampling and collects all metrics\"\"\"\n",
    "    \n",
    "    # Fit model once and get ORs\n",
    "    model = fit_logistic_regression(X_train, y_train)\n",
    "\n",
    "    \n",
    "    boot_auc, boot_bal_acc, boot_sens, boot_spec, boot_f1 = [], [], [], [], []\n",
    "    \n",
    "    # Stratified resampling (to ensure label distribution is maintained in each bootstrap sample)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_bootstrap, test_size=len(X_test)-5, random_state=42)\n",
    "\n",
    "    y_pred_probs_full = model.predict_proba(X_test)[:, 1]  # Get predicted probabilities for the full test set\n",
    "    if threshold is None:\n",
    "        threshold = find_best_threshold(y_test, y_pred_probs_full)\n",
    "        print(threshold)\n",
    "    \n",
    "    for _, test_index in sss.split(X_test, y_test):  # Splitting indices based on stratification\n",
    "        # Get the stratified bootstrap sample\n",
    "        sample_df_test = X_test.iloc[test_index]\n",
    "        sample_dep_test = y_test.iloc[test_index]\n",
    "        \n",
    "        # Get predicted probabilities for the resampled test data\n",
    "        y_pred_probs = model.predict_proba(sample_df_test)[:, 1]\n",
    "        \n",
    "        # Compute performance metrics\n",
    "        bal_acc, auc, f1, sens, spec = metrics(sample_dep_test, y_pred_probs, threshold)\n",
    "        \n",
    "        # Append metrics to the lists\n",
    "        boot_auc.append(auc)\n",
    "        boot_bal_acc.append(bal_acc)\n",
    "        boot_sens.append(sens)\n",
    "        boot_spec.append(spec)\n",
    "        boot_f1.append(f1)\n",
    "    \n",
    "    # Compute means and 95% Confidence Intervals\n",
    "    def ci(data):\n",
    "        return np.mean(data), np.percentile(data, [2.5, 97.5])\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"Bootstrapped Metrics\": {\n",
    "            \"AUC\": ci(boot_auc),\n",
    "            \"Balanced Accuracy\": ci(boot_bal_acc),\n",
    "            \"Sensitivity\": ci(boot_sens),\n",
    "            \"Specificity\": ci(boot_spec),\n",
    "            \"F1-score\": ci(boot_f1),\n",
    "        },\n",
    "        \"Arrays with bootstrapping\":{\n",
    "            \"AUC\": boot_auc,\n",
    "            \"Balanced Accuracy\": boot_bal_acc,\n",
    "            \"Sensitivity\": boot_sens,\n",
    "            \"Specificity\": boot_spec,\n",
    "            \"F1-score\": boot_f1,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def ready_for_test(df_path):\n",
    "    df = pd.read_csv(df_path)\n",
    "    df_marksheet = pd.read_csv(\"Data/Preprocessed_marksheet_all.csv\")\n",
    "    df_csPCa = df_marksheet.drop(columns=[\"study_id\", \"mri_date\", \"patient_age\", \"psa\", \"psad\", \"prostate_volume\", \"histopath_type\", \"lesion_GS\", \"lesion_ISUP\", \"case_ISUP\", \"center\"])\n",
    "    merge_df = pd.merge(df_csPCa, df, on=\"patient_id\", how=\"left\")\n",
    "    merge_df = merge_df.dropna()\n",
    "    y = merge_df[\"case_csPCa\"].values\n",
    "    merge_df = merge_df.drop(columns=[\"patient_id\", \"case_csPCa\"])\n",
    "    X = np.vstack(merge_df[\"combined_embedding\"].apply(lambda x: np.array(json.loads(x))).values)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    y_test = pd.Series(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def summarize_results(model_name, embeddings, context, feat_name, feat_expl, bootstrap_results):\n",
    "    summary = bootstrap_results[\"Bootstrapped Metrics\"]\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Embedding\": embeddings,\n",
    "        \"Context\": context,\n",
    "        \"Feature Name\": feat_name,\n",
    "        \"Feature Explanation\": feat_expl,\n",
    "        \"AUC (95% CI)\": f\"{summary['AUC'][0]:.3f} ({summary['AUC'][1][0]:.3f}-{summary['AUC'][1][1]:.3f})\",\n",
    "        \"Balanced Accuracy (95% CI)\": f\"{summary['Balanced Accuracy'][0]:.3f} ({summary['Balanced Accuracy'][1][0]:.3f}-{summary['Balanced Accuracy'][1][1]:.3f})\",\n",
    "        \"F1 (95% CI)\": f\"{summary['F1-score'][0]:.3f} ({summary['F1-score'][1][0]:.3f}-{summary['F1-score'][1][1]:.3f})\",\n",
    "        \"Sensitivity (95% CI)\": f\"{summary['Sensitivity'][0]:.3f} ({summary['Sensitivity'][1][0]:.3f}-{summary['Sensitivity'][1][1]:.3f})\",\n",
    "        \"Specificity (95% CI)\": f\"{summary['Specificity'][0]:.3f} ({summary['Specificity'][1][0]:.3f}-{summary['Specificity'][1][1]:.3f})\"\n",
    "    }\n",
    "\n",
    "\n",
    "def add_to_table(model_name, embeddings, context, feat_name, feat_expl, bootstrap_results):\n",
    "    summary = summarize_results(model_name, embeddings, context, feat_name, feat_expl, bootstrap_results)\n",
    "    df_table = pd.read_csv(\"Data/Results_table3.csv\")\n",
    "    df_summary = pd.DataFrame([summary])\n",
    "    df_table = pd.concat([df_table, df_summary], ignore_index=True)\n",
    "    df_table.to_csv(\"Data/Results_table3.csv\", index=False)\n",
    "\n",
    "def process(model, embeddings, guideline, feature_name, feature_context):\n",
    "    df = pd.read_csv(\"Data/Guidelines/guidelines_with_features_resampled2_bin5.csv\")\n",
    "    if guideline == \"Yes\" and feature_name == \"Yes\" and feature_context == \"Yes\":\n",
    "        if model == \"BioBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding(x[\"guideline\"] + \" \" + x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"ClinicalBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_CB(x[\"guideline\"] + \" \" + x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"BERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_Bert(x[\"guideline\"] + \" \" + x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"MedBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_MB(x[\"guideline\"] + \" \" + x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "    elif guideline == \"Yes\" and feature_name == \"Yes\" and feature_context == \"No\":\n",
    "        if model == \"BioBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding(x[\"guideline\"] + \" \" + x[\"feature_name\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"ClinicalBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_CB(x[\"guideline\"] + \" \" + x[\"feature_name\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"BERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_Bert(x[\"guideline\"] + \" \" + x[\"feature_name\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"MedBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_MB(x[\"guideline\"] + \" \" + x[\"feature_name\"], x[\"feature_value\"]), axis=1)\n",
    "    elif guideline == \"Yes\" and feature_name == \"No\" and feature_context == \"Yes\":\n",
    "        if model == \"BioBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding(x[\"guideline\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"ClinicalBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_CB(x[\"guideline\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"BERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_Bert(x[\"guideline\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"MedBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_MB(x[\"guideline\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "    elif guideline == \"Yes\" and feature_name == \"No\" and feature_context == \"No\":\n",
    "        if model == \"BioBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding(x[\"guideline\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"ClinicalBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_CB(x[\"guideline\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"BERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_Bert(x[\"guideline\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"MedBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_MB(x[\"guideline\"], x[\"feature_value\"]), axis=1)\n",
    "    elif guideline == \"No\" and feature_name == \"Yes\" and feature_context == \"Yes\":\n",
    "        if model == \"BioBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding(x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"ClinicalBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_CB(x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"BERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_Bert(x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"MedBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_MB(x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "    elif guideline == \"No\" and feature_name == \"Yes\" and feature_context == \"No\":\n",
    "        if model == \"BioBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding(x[\"feature_name\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"ClinicalBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_CB(x[\"feature_name\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"BERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_Bert(x[\"feature_name\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"MedBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_MB(x[\"feature_name\"], x[\"feature_value\"]), axis=1)\n",
    "    elif guideline == \"No\" and feature_name == \"No\" and feature_context == \"Yes\":\n",
    "        if model == \"BioBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding(x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"ClinicalBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_CB(x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"BERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_Bert(x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"MedBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_MB(x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "    elif guideline == \"No\" and feature_name == \"No\" and feature_context == \"No\":\n",
    "        if model == \"BioBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding(\"\", x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"ClinicalBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_CB(\"\", x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"BERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_Bert(\"\", x[\"feature_value\"]), axis=1)\n",
    "        elif model == \"MedBERT\":\n",
    "            embedding = df.apply(lambda x: get_embedding_MB(\"\", x[\"feature_value\"]), axis=1)\n",
    "    else:\n",
    "        print(\"Invalid combination of guideline, feature_name, and feature_context.\")\n",
    "\n",
    "    save_embeddings(df, embedding, f\"Data/embeddings/{model}_4/{model}_{guideline}_{feature_name}_{feature_context}.csv\")\n",
    "    X_train, X_test, y_train, y_test = ready_for_test(f\"Data/embeddings/{model}_4/{model}_{guideline}_{feature_name}_{feature_context}.csv\")\n",
    "    results = bootstrap_metrics(X_train, X_test, y_train, y_test, threshold=0.535, n_bootstrap=1000)\n",
    "    add_to_table(model, embeddings, guideline, feature_name, feature_context, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marksheet = pd.read_csv(\"Data/Preprocessed_marksheet_all.csv\")\n",
    "\n",
    "y = df_marksheet[\"case_csPCa\"].values\n",
    "X = df_marksheet.drop(columns=[\"study_id\", \"mri_date\", \"histopath_type\", \"lesion_GS\", \"lesion_ISUP\", \"case_ISUP\", \"center\", \"patient_id\", \"case_csPCa\"]).values\n",
    "X = pd.DataFrame(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.Series(y_test)\n",
    "bootstrap_results_model = bootstrap_metrics(X_train, X_test, y_train, y_test, threshold=0.535, n_bootstrap=1000)\n",
    "add_to_table(\"Clinical features\", \"No\", \"No\", \"No\", \"No\", bootstrap_results_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv(\"Data/Radiomic_features/Radiomic_features.csv\")\n",
    "df_marksheet = pd.read_csv(\"Data/Preprocessed_marksheet_all.csv\")\n",
    "\n",
    "df_csPCa = df_marksheet.drop(columns=[\"study_id\", \"mri_date\", \"patient_age\", \"psa\", \"psad\", \"prostate_volume\", \"histopath_type\", \"lesion_GS\", \"lesion_ISUP\", \"case_ISUP\", \"center\"])\n",
    "df_features.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "merge_df = pd.merge(df_csPCa, df_features, on=\"patient_id\", how=\"left\")\n",
    "merge_df = merge_df.dropna()\n",
    "y = merge_df[\"case_csPCa\"].values\n",
    "merge_df = merge_df.drop(columns=[\"patient_id\", \"case_csPCa\"])\n",
    "X_raw = merge_df.values\n",
    "df_X = pd.DataFrame(X_raw, columns=merge_df.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.Series(y_test)\n",
    "bootstrap_results_model = bootstrap_metrics(X_train, X_test, y_train, y_test, n_bootstrap=1000, threshold=0.535)\n",
    "add_to_table(\"Radiomic features\", \"No\", \"No\", \"No\", \"No\", bootstrap_results_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "278e9e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: Data/embeddings/BioBERT_4\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_4/BioBERT_Yes_Yes_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_4/BioBERT_Yes_Yes_No.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_4/BioBERT_Yes_No_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_4/BioBERT_Yes_No_No.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_4/BioBERT_No_Yes_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_4/BioBERT_No_Yes_No.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_4/BioBERT_No_No_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_4/BioBERT_No_No_No.csv\n"
     ]
    }
   ],
   "source": [
    "process(\"BioBERT\", \"Yes\", \"Yes\", \"Yes\", \"Yes\")\n",
    "process(\"BioBERT\", \"Yes\", \"Yes\", \"Yes\", \"No\")\n",
    "process(\"BioBERT\", \"Yes\", \"Yes\", \"No\", \"Yes\")\n",
    "process(\"BioBERT\", \"Yes\", \"Yes\", \"No\", \"No\")\n",
    "process(\"BioBERT\", \"Yes\", \"No\", \"Yes\", \"Yes\")\n",
    "process(\"BioBERT\", \"Yes\", \"No\", \"Yes\", \"No\")\n",
    "process(\"BioBERT\", \"Yes\", \"No\", \"No\", \"Yes\")\n",
    "process(\"BioBERT\", \"Yes\", \"No\", \"No\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "710fa38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: Data/embeddings/BERT_4\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BERT_4/BERT_Yes_Yes_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BERT_4/BERT_Yes_Yes_No.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BERT_4/BERT_Yes_No_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BERT_4/BERT_Yes_No_No.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BERT_4/BERT_No_Yes_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BERT_4/BERT_No_Yes_No.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BERT_4/BERT_No_No_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BERT_4/BERT_No_No_No.csv\n"
     ]
    }
   ],
   "source": [
    "process(\"BERT\", \"Yes\", \"Yes\", \"Yes\", \"Yes\")\n",
    "process(\"BERT\", \"Yes\", \"Yes\", \"Yes\", \"No\")\n",
    "process(\"BERT\", \"Yes\", \"Yes\", \"No\", \"Yes\")\n",
    "process(\"BERT\", \"Yes\", \"Yes\", \"No\", \"No\")\n",
    "process(\"BERT\", \"Yes\", \"No\", \"Yes\", \"Yes\")\n",
    "process(\"BERT\", \"Yes\", \"No\", \"Yes\", \"No\")\n",
    "process(\"BERT\", \"Yes\", \"No\", \"No\", \"Yes\")\n",
    "process(\"BERT\", \"Yes\", \"No\", \"No\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9aa7d928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: Data/embeddings/ClinicalBERT_4\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/ClinicalBERT_4/ClinicalBERT_Yes_Yes_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/ClinicalBERT_4/ClinicalBERT_Yes_Yes_No.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/ClinicalBERT_4/ClinicalBERT_Yes_No_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/ClinicalBERT_4/ClinicalBERT_Yes_No_No.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/ClinicalBERT_4/ClinicalBERT_No_Yes_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/ClinicalBERT_4/ClinicalBERT_No_Yes_No.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/ClinicalBERT_4/ClinicalBERT_No_No_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/ClinicalBERT_4/ClinicalBERT_No_No_No.csv\n"
     ]
    }
   ],
   "source": [
    "process(\"ClinicalBERT\", \"Yes\", \"Yes\", \"Yes\", \"Yes\")\n",
    "process(\"ClinicalBERT\", \"Yes\", \"Yes\", \"Yes\", \"No\")\n",
    "process(\"ClinicalBERT\", \"Yes\", \"Yes\", \"No\", \"Yes\")\n",
    "process(\"ClinicalBERT\", \"Yes\", \"Yes\", \"No\", \"No\")\n",
    "process(\"ClinicalBERT\", \"Yes\", \"No\", \"Yes\", \"Yes\")\n",
    "process(\"ClinicalBERT\", \"Yes\", \"No\", \"Yes\", \"No\")\n",
    "process(\"ClinicalBERT\", \"Yes\", \"No\", \"No\", \"Yes\")\n",
    "process(\"ClinicalBERT\", \"Yes\", \"No\", \"No\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b2507c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/MedBERT_4/MedBERT_No_Yes_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/MedBERT_4/MedBERT_No_Yes_No.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/MedBERT_4/MedBERT_No_No_Yes.csv\n",
      "Embedding length distribution: combined_embedding\n",
      "769    772\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/MedBERT_4/MedBERT_No_No_No.csv\n"
     ]
    }
   ],
   "source": [
    "process(\"MedBERT\", \"Yes\", \"Yes\", \"Yes\", \"Yes\")\n",
    "process(\"MedBERT\", \"Yes\", \"Yes\", \"Yes\", \"No\")\n",
    "process(\"MedBERT\", \"Yes\", \"Yes\", \"No\", \"Yes\")\n",
    "process(\"MedBERT\", \"Yes\", \"Yes\", \"No\", \"No\")\n",
    "process(\"MedBERT\", \"Yes\", \"No\", \"Yes\", \"Yes\")\n",
    "process(\"MedBERT\", \"Yes\", \"No\", \"Yes\", \"No\")\n",
    "process(\"MedBERT\", \"Yes\", \"No\", \"No\", \"Yes\")\n",
    "process(\"MedBERT\", \"Yes\", \"No\", \"No\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dceb338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length distribution: combined_embedding\n",
      "14611    1324\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_3/BioBERT_No_No_No.csv\n"
     ]
    }
   ],
   "source": [
    "process(\"BioBERT\", \"Yes\", \"No\", \"No\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e88c7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length distribution: combined_embedding\n",
      "14611    1324\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_3/BioBERT_MaxAbs.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"guidelines_scaled_features_MaxAbsScaler.csv\")\n",
    "emb = df.apply(lambda x: get_embedding(x[\"guideline\"] + \" \" + x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "save_embeddings(df, emb, \"Data/embeddings/BioBERT_3/BioBERT_MaxAbs.csv\")\n",
    "X_train, X_test, y_train, y_test = ready_for_test(f\"Data/embeddings/BioBERT_3/BioBERT_MaxAbs.csv\")\n",
    "results = bootstrap_metrics(X_train, X_test, y_train, y_test, threshold=0.535, n_bootstrap=1000)\n",
    "add_to_table(\"BioBERT\", \"Yes\", \"Yes\", \"Yes\", \"Yes_MaxAbs\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "330de4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length distribution: combined_embedding\n",
      "14611    1324\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_3/BioBERT_MinMax.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"guidelines_scaled_features_MinMaxScaler.csv\")\n",
    "emb = df.apply(lambda x: get_embedding(x[\"guideline\"] + \" \" + x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "save_embeddings(df, emb, \"Data/embeddings/BioBERT_3/BioBERT_MinMax.csv\")\n",
    "X_train, X_test, y_train, y_test = ready_for_test(f\"Data/embeddings/BioBERT_3/BioBERT_MinMax.csv\")\n",
    "results = bootstrap_metrics(X_train, X_test, y_train, y_test, threshold=0.535, n_bootstrap=1000)\n",
    "add_to_table(\"BioBERT\", \"Yes\", \"Yes\", \"Yes\", \"Yes_MinMax\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b7e9246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length distribution: combined_embedding\n",
      "14611    1324\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_3/BioBERT_Robust.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"guidelines_scaled_features_RobustScaler.csv\")\n",
    "emb = df.apply(lambda x: get_embedding(x[\"guideline\"] + \" \" + x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "save_embeddings(df, emb, \"Data/embeddings/BioBERT_3/BioBERT_Robust.csv\")\n",
    "X_train, X_test, y_train, y_test = ready_for_test(f\"Data/embeddings/BioBERT_3/BioBERT_Robust.csv\")\n",
    "results = bootstrap_metrics(X_train, X_test, y_train, y_test, threshold=0.535, n_bootstrap=1000)\n",
    "add_to_table(\"BioBERT\", \"Yes\", \"Yes\", \"Yes\", \"Yes_Robust\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2db94b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length distribution: combined_embedding\n",
      "14611    1324\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_3/BioBERT_Standard.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"guidelines_scaled_features_StandardScaler.csv\")\n",
    "emb = df.apply(lambda x: get_embedding(x[\"guideline\"] + \" \" + x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "save_embeddings(df, emb, \"Data/embeddings/BioBERT_3/BioBERT_Standard.csv\")\n",
    "X_train, X_test, y_train, y_test = ready_for_test(f\"Data/embeddings/BioBERT_3/BioBERT_Standard.csv\")\n",
    "results = bootstrap_metrics(X_train, X_test, y_train, y_test, threshold=0.535, n_bootstrap=1000)\n",
    "add_to_table(\"BioBERT\", \"Yes\", \"Yes\", \"Yes\", \"Yes_Standard\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "57e89a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length distribution: combined_embedding\n",
      "14611    1324\n",
      "Name: count, dtype: int64\n",
      "Embeddings saved to Data/embeddings/BioBERT_3/BioBERT_rounded.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"guidelines_features_rounded.csv\")\n",
    "emb = df.apply(lambda x: get_embedding(x[\"guideline\"] + \" \" + x[\"feature_name\"] + \" \" + x[\"feature_context\"], x[\"feature_value\"]), axis=1)\n",
    "save_embeddings(df, emb, \"Data/embeddings/BioBERT_3/BioBERT_rounded.csv\")\n",
    "X_train, X_test, y_train, y_test = ready_for_test(f\"Data/embeddings/BioBERT_3/BioBERT_rounded.csv\")\n",
    "results = bootstrap_metrics(X_train, X_test, y_train, y_test, threshold=0.535, n_bootstrap=1000)\n",
    "add_to_table(\"BioBERT\", \"Yes\", \"Yes\", \"Yes\", \"Yes_Rounded\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "77333b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/removed_low_correlation_features_bin5.csv\")\n",
    "df_marksheet = pd.read_csv(\"Data/Preprocessed_marksheet_all.csv\")\n",
    "\n",
    "df_csPCa = df_marksheet.drop(columns=[\"study_id\", \"mri_date\", \"patient_age\", \"psa\", \"psad\", \"prostate_volume\", \"histopath_type\", \"lesion_GS\", \"lesion_ISUP\", \"case_ISUP\", \"center\"])\n",
    "\n",
    "merge_df = pd.merge(df_csPCa, df, on=\"patient_id\", how=\"left\")\n",
    "merge_df = merge_df.dropna()\n",
    "y = merge_df[\"case_csPCa\"].values\n",
    "merge_df = merge_df.drop(columns=[\"patient_id\", \"case_csPCa\"])\n",
    "X_raw = merge_df.values\n",
    "df_X = pd.DataFrame(X_raw, columns=merge_df.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.Series(y_test)\n",
    "bootstrap_results_model = bootstrap_metrics(X_train, X_test, y_train, y_test, n_bootstrap=1000, threshold=0.535)\n",
    "add_to_table(\"Radiomic features_Removedlowcorr\", \"No\", \"No\", \"No\", \"No\", bootstrap_results_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "60699a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"scaled_radiomic_features_MaxAbsScaler.csv\")\n",
    "df_marksheet = pd.read_csv(\"Data/Preprocessed_marksheet_all.csv\")\n",
    "\n",
    "df_csPCa = df_marksheet.drop(columns=[\"study_id\", \"mri_date\", \"patient_age\", \"psa\", \"psad\", \"prostate_volume\", \"histopath_type\", \"lesion_GS\", \"lesion_ISUP\", \"case_ISUP\", \"center\"])\n",
    "\n",
    "merge_df = pd.merge(df_csPCa, df, on=\"patient_id\", how=\"left\")\n",
    "merge_df = merge_df.dropna()\n",
    "y = merge_df[\"case_csPCa\"].values\n",
    "merge_df = merge_df.drop(columns=[\"patient_id\", \"case_csPCa\"])\n",
    "X_raw = merge_df.values\n",
    "df_X = pd.DataFrame(X_raw, columns=merge_df.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.Series(y_test)\n",
    "bootstrap_results_model = bootstrap_metrics(X_train, X_test, y_train, y_test, n_bootstrap=1000, threshold=0.535)\n",
    "add_to_table(\"Radiomic features_Removedlowcorr\", \"No\", \"No\", \"No\", \"No\", bootstrap_results_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
